{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read csv into a dataframe, focus on tweets\n",
    "df = pd.read_csv(\"Tweets.csv\", header=None, usecols=[1])\n",
    "na = np.array(df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase all words to level the playing field\n",
    "nb = []\n",
    "for x in na:\n",
    "    ar =[]\n",
    "    for y in x:\n",
    "        ar.append(y.lower())\n",
    "    nb.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize \n",
    "nc = []\n",
    "for x in nb:\n",
    "    for y in x:\n",
    "        nc.append(word_tokenize(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "nd = []\n",
    "stopWords = stopwords.words('english')\n",
    "for x in nc:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        if y not in stopWords:\n",
    "            ar.append(y)\n",
    "    nd.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations\n",
    "ne = []\n",
    "for x in nd:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        ar.append(y.translate(str.maketrans('', '', string.punctuation)))\n",
    "    ne.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up to remove single character members and empty list values\n",
    "nf = []\n",
    "for x in ne:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        if len(y) > 1:\n",
    "            ar.append(y)\n",
    "    nf.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check for ascii characters\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non ascii characters\n",
    "ng = []\n",
    "for x in nf:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        c = \"\"\n",
    "        for q in y:\n",
    "            c = c + (q if is_ascii(q) else \" \")\n",
    "        ar.append(c)\n",
    "    ng.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove html characters\n",
    "nh = []\n",
    "for x in ng:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        ar.append(re.compile(r'<[^>]+>').sub('', y))\n",
    "    nh.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove emoji\n",
    "ni = []\n",
    "for x in nh:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        ar.append(re.sub(emoji.get_emoji_regexp(), r\"\", y))\n",
    "    ni.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd row of cleaning remove numbers and any characters which may contain  numbers\n",
    "import numbers\n",
    "nj = []\n",
    "for x in ni:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        if not y.isdigit():\n",
    "            if not bool(re.search(r'\\d', y)):\n",
    "                ar.append(y)\n",
    "    nj.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3rd round of cleaning, remove empty array elements caused by removal of html characters\n",
    "nk = []\n",
    "for x in nj:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        if len(y) > 1:\n",
    "            ar.append(y)\n",
    "    nk.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematize words using nltk\n",
    "nl = []\n",
    "for x in nk:\n",
    "    ar = []\n",
    "    for y in x:\n",
    "        ar.append(lemma.lemmatize(y))\n",
    "    nl.append(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = []\n",
    "for x in nl:\n",
    "    nm.append(pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [('comedycentralke', 'NN'), ('word', 'NN'), ('kill', 'VB'), ('luhya', 'JJ'), ('wacha', 'NN'), ('ugali', 'JJ'), ('ipoe', 'NN')]),\n",
       " Tree('S', [('little', 'JJ'), ('luhya', 'NN'), ('remains', 'VBZ'), ('always', 'RB'), ('sitokingi', 'VBN'), ('kill', 'NN')]),\n",
       " Tree('S', [('cheernatwildcat', 'NN'), ('kill', 'VB'), ('battle', 'NN'), ('weekend', 'NN'), ('wildcat', 'NN'), ('luhya', 'NN')]),\n",
       " Tree('S', [('homeboyzradio', 'NN'), ('hbr', 'NN'), ('luv', 'NN'), ('dat', 'NN'), ('luhya', 'NN'), ('hit', 'VBD'), ('luhyas', 'JJ'), ('always', 'RB'), ('kill', 'VBP'), ('awoooh', 'NNS')]),\n",
       " Tree('S', [('hangoutfriday', 'NN'), ('hahaha', 'VB'), ('ball', 'NN'), ('ya', 'NN'), ('terby', 'JJ'), ('derby', 'JJ'), ('luhyas', 'NN'), ('kill', 'NN')]),\n",
       " Tree('S', [('luo', 'NN'), ('kill', 'VB'), ('blood', 'NN'), ('thirsty', 'JJ'), ('killer', 'NN'), ('two', 'CD'), ('tribe', 'NN')]),\n",
       " Tree('S', [('police', 'NNS'), ('kill', 'MD'), ('luo', 'VB'), ('bondo', 'NN'), ('shot', 'NN'), ('granny', 'NN'), ('migori', 'NN')]),\n",
       " Tree('S', [('provision', 'NN'), ('police', 'NN'), ('kill', 'VB'), ('innocent', 'NN'), ('unarmed', 'JJ'), ('luo', 'NN')]),\n",
       " Tree('S', [('gvnt', 'NN'), ('u', 'NN'), ('determined', 'VBD'), ('kill', 'NNP'), ('luo', 'CC'), ('young', 'JJ'), ('old', 'JJ'), ('issfine', 'NN')]),\n",
       " Tree('S', [('today', 'NN'), ('event', 'NN'), ('scok', 'NN'), ('maraga', 'NN'), ('adventist', 'NN'), ('uphold', 'JJ'), ('uhuru', 'NN')])]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Named Entity recognition\n",
    "import nltk\n",
    "nn = []\n",
    "for x in nm:\n",
    "    nn.append(nltk.ne_chunk(x, binary=False))\n",
    "nn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
